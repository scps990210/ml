import nltk
from nltk.corpus import movie_reviews
import random
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
import pickle
import re

def preprocess_text(text):
    """æ”¹é€²çš„æ–‡å­—é è™•ç†"""
    # è½‰å°å¯«
    text = text.lower()
    
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä½†ä¿ç•™é‡è¦æ¨™é»ç¬¦è™Ÿ
    text = re.sub(r'[^a-zA-Z0-9\s!?.]', '', text)
    
    # ç§»é™¤å¤šé¤˜ç©ºç™½
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def train_sentiment_model():
    """æ”¹é€²ç‰ˆæƒ…æ„Ÿåˆ†ææ¨¡å‹è¨“ç·´"""
    
    print("ğŸš€ é–‹å§‹è¨“ç·´æ”¹é€²ç‰ˆæƒ…æ„Ÿåˆ†ææ¨¡å‹...")
    
    # ä¸‹è¼‰è³‡æ–™é›†
    try:
        nltk.download("movie_reviews", quiet=True)
        nltk.download("punkt", quiet=True)
        nltk.download("stopwords", quiet=True)
        print("âœ“ NLTK è³‡æ–™é›†æº–å‚™å®Œæˆ")
    except:
        print("âš  NLTK è³‡æ–™é›†ä¸‹è¼‰å¤±æ•—ï¼Œä½†å¯èƒ½å·²å­˜åœ¨")

    # æº–å‚™è³‡æ–™
    print("ğŸ“š è¼‰å…¥ä¸¦é è™•ç†é›»å½±è©•è«–è³‡æ–™...")
    documents = [(list(movie_reviews.words(fileid)), category)
                 for category in movie_reviews.categories()
                 for fileid in movie_reviews.fileids(category)]

    # è¨­å®šéš¨æ©Ÿç¨®å­ç¢ºä¿å¯é‡ç¾æ€§
    random.seed(42)
    np.random.seed(42)
    random.shuffle(documents)

    print(f"ç¸½å…±è¼‰å…¥ {len(documents)} ç­†è©•è«–è³‡æ–™")

    # æ”¹é€²çš„æ–‡å­—é è™•ç†
    texts = []
    labels = []
    
    for words, label in documents:
        # é‡å»ºæ–‡å­—ä¸¦é€²è¡Œé è™•ç†
        text = " ".join(words)
        processed_text = preprocess_text(text)
        
        # éæ¿¾æ‰å¤ªçŸ­çš„æ–‡å­—ï¼ˆå¯èƒ½æ˜¯é›œè¨Šï¼‰
        if len(processed_text.split()) > 10:
            texts.append(processed_text)
            labels.append(1 if label == "pos" else 0)

    print(f"é è™•ç†å¾Œä¿ç•™ {len(texts)} ç­†æœ‰æ•ˆè³‡æ–™")

    # æª¢æŸ¥è³‡æ–™å¹³è¡¡åº¦
    pos_count = sum(labels)
    neg_count = len(labels) - pos_count
    print(f"æ­£é¢è©•è«–: {pos_count} ç­† ({pos_count/len(labels)*100:.1f}%)")
    print(f"è² é¢è©•è«–: {neg_count} ç­† ({neg_count/len(labels)*100:.1f}%)")

    # æ”¹ç”¨ TF-IDF å‘é‡åŒ–å™¨ï¼ˆæ¯” CountVectorizer æ›´å¥½ï¼‰
    print("ğŸ”§ ä½¿ç”¨ TF-IDF é€²è¡Œç‰¹å¾µå·¥ç¨‹...")
    vectorizer = TfidfVectorizer(
        max_features=5000,           # å¢åŠ ç‰¹å¾µæ•¸é‡
        stop_words='english',        # ç§»é™¤è‹±æ–‡åœç”¨è©
        ngram_range=(1, 3),         # ä½¿ç”¨ 1-gram, 2-gram, 3-gram
        min_df=3,                   # è©å½™è‡³å°‘å‡ºç¾3æ¬¡
        max_df=0.8,                 # ç§»é™¤å‡ºç¾åœ¨è¶…é80%æ–‡æª”ä¸­çš„è©
        sublinear_tf=True,          # ä½¿ç”¨æ¬¡ç·šæ€§TFç¸®æ”¾
        use_idf=True,               # ä½¿ç”¨IDFæ¬Šé‡
        smooth_idf=True,            # å¹³æ»‘IDF
        norm='l2'                   # L2æ­£è¦åŒ–
    )

    # åˆ‡åˆ†è³‡æ–™é›†
    X_train, X_test, y_train, y_test = train_test_split(
        texts, labels, test_size=0.2, random_state=42, stratify=labels
    )

    # å»ºç«‹æ”¹é€²çš„æ©Ÿå™¨å­¸ç¿’ç®¡é“
    print("ğŸ¤– å»ºç«‹ä¸¦è¨“ç·´æ”¹é€²çš„æ©Ÿå™¨å­¸ç¿’æ¨¡å‹...")
    
    # ä½¿ç”¨ç®¡é“çµ„åˆå‘é‡åŒ–å’Œæ¨¡å‹è¨“ç·´
    pipeline = Pipeline([
        ('tfidf', vectorizer),
        ('classifier', LogisticRegression(
            max_iter=3000,
            random_state=42,
            class_weight='balanced',    # è‡ªå‹•å¹³è¡¡é¡åˆ¥æ¬Šé‡
            solver='liblinear',         # é©åˆå°åˆ°ä¸­ç­‰è³‡æ–™é›†
            C=1.0                       # æ­£è¦åŒ–å¼·åº¦
        ))
    ])

    # ç¶²æ ¼æœç´¢æœ€ä½³åƒæ•¸
    print("ğŸ” é€²è¡Œè¶…åƒæ•¸å„ªåŒ–...")
    param_grid = {
        'tfidf__max_features': [3000, 5000, 7000],
        'tfidf__ngram_range': [(1, 2), (1, 3)],
        'classifier__C': [0.1, 1.0, 10.0],
        'classifier__solver': ['liblinear', 'lbfgs']
    }

    # ä½¿ç”¨ç¶²æ ¼æœç´¢æ‰¾æœ€ä½³åƒæ•¸ï¼ˆè¼ƒå¿«çš„ç‰ˆæœ¬ï¼‰
    grid_search = GridSearchCV(
        pipeline, param_grid, 
        cv=3,                    # 3æŠ˜äº¤å‰é©—è­‰
        scoring='f1_macro',      # ä½¿ç”¨F1åˆ†æ•¸å¹³è¡¡ç²¾ç¢ºåº¦å’Œå¬å›ç‡
        n_jobs=-1,              # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
        verbose=1               # é¡¯ç¤ºé€²åº¦
    )

    # è¨“ç·´æ¨¡å‹
    grid_search.fit(X_train, y_train)
    
    # å–å¾—æœ€ä½³æ¨¡å‹
    best_model = grid_search.best_estimator_
    print(f"âœ“ æ‰¾åˆ°æœ€ä½³åƒæ•¸: {grid_search.best_params_}")

    # è©•ä¼°æ¨¡å‹
    train_accuracy = best_model.score(X_train, y_train)
    test_accuracy = best_model.score(X_test, y_test)

    print(f"\nğŸ¯ æ¨¡å‹è¨“ç·´å®Œæˆï¼")
    print(f"è¨“ç·´é›†æº–ç¢ºç‡: {train_accuracy:.4f}")
    print(f"æ¸¬è©¦é›†æº–ç¢ºç‡: {test_accuracy:.4f}")
    print(f"æœ€ä½³äº¤å‰é©—è­‰åˆ†æ•¸: {grid_search.best_score_:.4f}")

    # è©³ç´°è©•ä¼°
    y_pred = best_model.predict(X_test)
    print("\nğŸ“Š è©³ç´°è©•ä¼°å ±å‘Š:")
    print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))
    
    print("\nğŸ”¢ æ··æ·†çŸ©é™£:")
    cm = confusion_matrix(y_test, y_pred)
    print(f"çœŸè² ä¾‹(TN): {cm[0][0]}, å‡æ­£ä¾‹(FP): {cm[0][1]}")
    print(f"å‡è² ä¾‹(FN): {cm[1][0]}, çœŸæ­£ä¾‹(TP): {cm[1][1]}")

    # å„²å­˜å®Œæ•´çš„ç®¡é“ï¼ˆåŒ…å«å‘é‡åŒ–å™¨å’Œæ¨¡å‹ï¼‰
    print("\nğŸ’¾ å„²å­˜æ¨¡å‹...")
    try:
        # å„²å­˜å®Œæ•´ç®¡é“
        pickle.dump(best_model, open("sentiment_pipeline.pkl", "wb"))
        
        # ç‚ºäº†ç›¸å®¹æ€§ï¼Œä¹Ÿå„²å­˜åˆ†é›¢çš„çµ„ä»¶
        fitted_vectorizer = best_model.named_steps['tfidf']
        fitted_classifier = best_model.named_steps['classifier']
        
        pickle.dump(fitted_classifier, open("model.pkl", "wb"))
        pickle.dump(fitted_vectorizer, open("vectorizer.pkl", "wb"))
        
        print("âœ“ æ¨¡å‹å·²å„²å­˜ç‚º:")
        print("  - sentiment_pipeline.pkl (å®Œæ•´ç®¡é“)")
        print("  - model.pkl å’Œ vectorizer.pkl (åˆ†é›¢çµ„ä»¶)")
    except Exception as e:
        print(f"âœ— æ¨¡å‹å„²å­˜å¤±æ•—: {e}")
        return False

    # å…¨é¢æ¸¬è©¦æ¨¡å‹
    print("\nğŸ§ª å…¨é¢æ¸¬è©¦æ¨¡å‹é æ¸¬åŠŸèƒ½...")
    test_samples = [
        ("This movie is absolutely amazing and fantastic!", "æ‡‰è©²æ˜¯æ­£é¢"),
        ("Terrible film, complete waste of time and money.", "æ‡‰è©²æ˜¯è² é¢"),
        ("I love this movie so much! Great acting and plot.", "æ‡‰è©²æ˜¯æ­£é¢"),
        ("Boring, awful, and poorly made. I hate it.", "æ‡‰è©²æ˜¯è² é¢"),
        ("Pretty good movie, enjoyed watching it.", "æ‡‰è©²æ˜¯æ­£é¢"),
        ("Not bad, but not great either. Average movie.", "å¯èƒ½æ˜¯ä¸­æ€§åæ­£é¢"),
        ("Excellent cinematography and outstanding performances!", "æ‡‰è©²æ˜¯æ­£é¢"),
        ("Disappointed. Expected much better. Waste of time.", "æ‡‰è©²æ˜¯è² é¢"),
    ]

    correct_predictions = 0
    total_predictions = len(test_samples)

    for text, expected in test_samples:
        try:
            result = best_model.predict([text])[0]
            prob = best_model.predict_proba([text])[0]
            prediction = "Positive ğŸ˜€" if result == 1 else "Negative ğŸ˜¡"
            confidence = max(prob)
            
            print(f"\næ–‡æœ¬: {text}")
            print(f"é æœŸ: {expected}")
            print(f"é æ¸¬: {prediction} (ä¿¡å¿ƒåº¦: {confidence:.3f})")
            
            # ç°¡å–®åˆ¤æ–·é æ¸¬æ˜¯å¦åˆç†
            is_correct = (result == 1 and "æ­£é¢" in expected) or (result == 0 and "è² é¢" in expected)
            if is_correct:
                correct_predictions += 1
                print("âœ“ é æ¸¬åˆç†")
            else:
                print("? é æ¸¬å¯èƒ½éœ€è¦æª¢è¦–")
                
        except Exception as e:
            print(f"âœ— é æ¸¬å¤±æ•—: {e}")

    print(f"\nğŸ“ˆ æ¸¬è©¦æ¨£æœ¬é æ¸¬æº–ç¢ºç‡: {correct_predictions/total_predictions*100:.1f}%")

    # åˆ†æé‡è¦ç‰¹å¾µ
    print("\nğŸ” åˆ†ææœ€é‡è¦çš„ç‰¹å¾µè©å½™...")
    try:
        feature_names = fitted_vectorizer.get_feature_names_out()
        coef = fitted_classifier.coef_[0]
        
        # æœ€æ­£é¢çš„è©å½™
        pos_indices = coef.argsort()[-20:][::-1]
        print("æœ€æ­£é¢çš„ç‰¹å¾µ:", [feature_names[i] for i in pos_indices])
        
        # æœ€è² é¢çš„è©å½™
        neg_indices = coef.argsort()[:20]
        print("æœ€è² é¢çš„ç‰¹å¾µ:", [feature_names[i] for i in neg_indices])
    except:
        print("ç„¡æ³•åˆ†æç‰¹å¾µè©å½™")

    return True

def test_model_balance():
    """æ¸¬è©¦æ¨¡å‹æ˜¯å¦æœ‰åå‘å•é¡Œ"""
    print("\nğŸ² æ¸¬è©¦æ¨¡å‹å¹³è¡¡æ€§...")
    
    try:
        # è¼‰å…¥ç®¡é“
        pipeline = pickle.load(open("sentiment_pipeline.pkl", "rb"))
        
        # æ¸¬è©¦æ˜é¡¯çš„æ­£è² é¢å¥å­
        obvious_positive = [
            "amazing wonderful excellent fantastic",
            "love great best awesome incredible",
            "perfect brilliant outstanding superb"
        ]
        
        obvious_negative = [
            "terrible awful horrible disgusting",
            "hate worst boring stupid",
            "disappointment waste trash garbage"
        ]
        
        pos_correct = 0
        for text in obvious_positive:
            pred = pipeline.predict([text])[0]
            if pred == 1:
                pos_correct += 1
        
        neg_correct = 0
        for text in obvious_negative:
            pred = pipeline.predict([text])[0]
            if pred == 0:
                neg_correct += 1
        
        print(f"æ˜é¡¯æ­£é¢å¥å­é æ¸¬æ­£ç¢º: {pos_correct}/{len(obvious_positive)}")
        print(f"æ˜é¡¯è² é¢å¥å­é æ¸¬æ­£ç¢º: {neg_correct}/{len(obvious_negative)}")
        
        if pos_correct == 0:
            print("âš  è­¦å‘Šï¼šæ¨¡å‹å¯èƒ½æœ‰åš´é‡çš„è² é¢åå‘ï¼")
        elif neg_correct == 0:
            print("âš  è­¦å‘Šï¼šæ¨¡å‹å¯èƒ½æœ‰åš´é‡çš„æ­£é¢åå‘ï¼")
        else:
            print("âœ“ æ¨¡å‹å¹³è¡¡æ€§çœ‹èµ·ä¾†æ­£å¸¸")
            
    except Exception as e:
        print(f"å¹³è¡¡æ€§æ¸¬è©¦å¤±æ•—: {e}")

if __name__ == "__main__":
    print("="*60)
    print("ğŸ¯ æ”¹é€²ç‰ˆæƒ…æ„Ÿåˆ†ææ¨¡å‹è¨“ç·´ç³»çµ±")
    print("="*60)
    
    success = train_sentiment_model()
    
    if success:
        test_model_balance()
        
        print("\n" + "="*60)
        print("âœ… æ¨¡å‹è¨“ç·´å®Œæˆï¼ä¸»è¦æ”¹é€²:")
        print("âœ“ ä½¿ç”¨ TF-IDF æ›¿ä»£ç°¡å–®è¨ˆæ•¸")
        print("âœ“ åŠ å…¥é¡åˆ¥å¹³è¡¡æ¬Šé‡")
        print("âœ“ è¶…åƒæ•¸ç¶²æ ¼æœç´¢å„ªåŒ–")
        print("âœ“ æ”¹é€²çš„æ–‡å­—é è™•ç†")
        print("âœ“ æ›´å…¨é¢çš„æ¨¡å‹è©•ä¼°")
        print("\nç¾åœ¨æ‚¨å¯ä»¥åŸ·è¡Œ app.py ä¾†å•Ÿå‹•ç¶²é æ‡‰ç”¨ç¨‹å¼")
        print("="*60)
    else:
        print("\nâŒ æ¨¡å‹è¨“ç·´å¤±æ•—ï¼è«‹æª¢æŸ¥éŒ¯èª¤è¨Šæ¯")
